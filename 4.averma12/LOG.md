# -100DaysofMLCode
This is 100 days of ML code in collaboration with the folks at Mumbai School of AI. It will complement my work
in my own repo github.com/averma12

# Day 1
 Revised on file operations in Python using the os and pathlib modules.
 Also downloaded zip using python and unpacked it. Took reference from the fast.ai library.
 Did eda on data downloaded.
 Day1 trying to do Facial Kepypoints Detection project again from scratch using Pytorch.
 https://github.com/averma12/Beginning_Data_Science/blob/master/Download%20and%20unpack%20requests.ipynb
 
 # Day 2
  Read on Data Augmentation techniques and Image Transforms. Looked at various forms of Data Augmentation techniques. tried to implement some of them in facial keypoints detection. Still working on them. Will update github
   link when complete.
   Links referred to
   https://blog.paperspace.com/data-augmentation-for-bounding-boxes/
   https://stackoverflow.com/questions/49659236/rotating-keypoints-numpy-array
   
# Day 3
      Built on day 2 of Data Augmentation techniques.
      Read on histogram equalization,mormalization,Gaussian blurring.
      Found out limitations of techniques I can apply on current project.
      Onwards to day 4 , defining model, data loader.
      https://github.com/averma12/Beginning_Data_Science/blob/master/View%20Image%20and%20Transforms.ipynb
      
# Day 4
     Introduction to datasets and dataloaders in pytorch. Combining them with transforms.
     Also introduced to databunch in fast.ai. Will look more into it tomorrow

# Day 5
    Read about dataloaders and datasets . Read about fast.ai databunch which is a combination of a dataloader and transforms for passing onto the model. Implementing a MNIST example using a Resblock and the databunch API. To use that later for facial keypoints project
    
    Key Points
    DataLoaders.
    Class Objects and class methods.
    Using on MNIST example.
    Code to follow soon
    
# Day 6
  https://github.com/averma12/Deep-Learning/blob/master/MNIST_ResBlocks.ipynb
  Code completed.
  
# Day 7
     Travelling Rest Day


# Day 8,9 (10-11 Jan)
  Travelling so no progress


# Day 10
 Completed Type hinting in Python.
 https://github.com/averma12/Beginning_Data_Science/blob/master/Type%20Hinting%20in%20Python.ipynb
 
 
# Day 11
 Generators and Iterators in Pyhton. Building blocks of dataloaders and datasets. Completed iterator class implementation. Doing generators now.
 
# Day 12
Generators. Data processing with pandas revision.
Signed up to be School of AI dean . Hope for the best

# Day 13
Rest

# Day 14
  Bayes Theorem. Bayesian classification to classify documents based on tags
  https://towardsdatascience.com/tagging-documents-based-on-important-words-92bee9baa310
  
  
  
  
# Day 15
  NLP nanodegree udacity regex , beautifulsoup and text normalization
  
# Day 16
  Learnt about bayes theorem.
  Extended bayes theorem in the naive bayes algorithm.
  Applied naive bayes from scratch on a sample problem and then using sklearn
  https://github.com/averma12/Deep-Learning/tree/master/Machine_Learning_algorithms_scratch/Naive%20Bayes
  
# Day 17
   Read about Hidden Markov Models
   Viterbi algorithm
   Intro to dynamic programming
   
   
# Day 18
   Read in detail about Viterbi algorithm
   Used it on an example
   Cached recursion in dynamic programming
   Itertools library in python
   Used new dunder method to create singleton in pyhton
   
# Day 19
Reviewed word2vec and word embeddings .
need to do more work on this and finsih markov model project

# Day 20
Reviewed word embeddings and algorithms of word2vec
Formulated idea for finishing in next couple of days.
No progress made on Markov model
Need to definitely make progress tomorrow

# Day 21
Made headway in markov model project of udacity NLP DL
Revised collections. 
Made Lookup table

# Day 22
Revised Word2vec algorithm with Siraj's video using GOT books
Revised PCA.

# Day 23
Sentiment analysis with Siraj at meetup
Discussion on future of AI and projects
NLP nanodegree of udacity lookup table,bigrams implementation

# Day 24
Completed HMM of NLP DL.
Reviewed Project
Introduction to Probabalistic Programming
Sumbitted Project

# Day 25
Slow day. Clarified misconceptions on zip function and iterators and generators
Started storytelling with data for data visualization

# Day 26
Data visualization fundamentals
Different data types used in visualization
Different Libraries
Different types of data
D3.js basics
How to import kaggle data into Google Colab

# Day 27
Data visualization with D3.js
EDA on NYC taxi fare with Google colab
Loaded 55 million rows
Read about dask
Next onto applying the RF algorithm

# Day 28
Intro to Latent Dirichlet Allocation
Comparison with TF-IDF
Read about dirichlet distribution
Alpha values and beta values
Difference between BOW and LDA matrices

# Day 29
Revision of NLP fundamentals
Regex
Tokenization
NLTK
Spacy fundamentals

# Day 30
Continued revsion of NLP fundamentals
Revised Matplotlib
Algorithm of LDA
time to make paper to code

# Day 31
EDA of kaggle dataset using pandas and seaborn
Completed seaborn mini course on codecademy.
Doing LDA for kaggle dataset
Finished parts of resume for Udacity propel

# Day 32
Finished implementation of Kaggle Project using LDA
Resutls need reviewing
Also algorithm took too long to execute from scratch
Reviesed NMF algorithm
Look to implement same using NMF

# Day 33
Travelled for Job interviews.
Revised LDA,NMF for job interviews
Applied to be mentor for udacity (Pending video recording)
Revised Batch Norm
Revised Resnets

# Day 34
Introduction to RNN
RNN forward prop
RNN Back propagation through time
RNN  Keras API
RNN on toy example

# Day 35
Revised RNN
Example of Seq2Seq networks from Andrew Trasks Grokking Deep Learning with BackProp
Theory on Embedding and Visualizing embedding
Introduction to Homomorphism

# Day 36
Encoder decoder backprop revise
Attention introduction
DS India meetup Image augmentation

# Day 37
Not much with respect to deep learning
Deep Learning Book chapter 2 Linear Algebra chapter
Prepared notes.
Preparing jupyter notebooks
https://hadrienj.github.io/deep-learning-book-series-home/
Started StoryTelling with data by Cole Nussbaummer Knafflic

# Day 38
Rest day


# Day 39
Finished Linear Algebra chapter completely
Derived svd and PCA from scratch

# Day 40
Randomized SVD from scratch
Limitations of NMF.
Seq2Seq revision
Introduction to attention mechanism
Need to revise LSTMs again.
Need to see how LR finder can be applied.

# Day 41
Attention mechanism
Multiplicative attention and Bahdanau Attention basics
Implemented a toy example on Multiplicative attention
Learning about 1-cycle policy
Started Machine translation project of udacity NLP ND

# Day 42
Learnt the inner workings of nn frameworks by building a toy framework.

# Day 43
Learnt how to create and forward propagate word embeddings for RNN
And how to define Sigmoid and Tanh layers for RNN and LSTM.
Swift India meetup
Learnt about protocols and generics

# Day 44-45
Learnt about inner workings of RNN and LSTM
Learnt about the steps used in a framework to train a model
Trained vanilla LSTM using keras for practice
Training with ensemble learning and CLR using keras.
Will apply to seq2seq with attention for udacity project

# Day 46
Encoder-Decoder architecture in keras using Keras functional APO
Learnt how to implement CLR and SGDR and cosine annealing in Keras
Completed 20% of udacity NLP nd project 2
  
  
  
# Day 47
Completed theory of seq2seq
Started probability from deep learning book

# Day 48
Probability chapter and lessons from udacity

# Day 49
Statistics , Binomial Distributions
Linear Regression

# Day 50
Correlation,Covariance
PCA,
Regression
Lasso Rigression,Ridge Regression,Linear Regression

# Day 51
Concurrency,Parallelism,Multi-Threading

# Day 52
Started projects Hinglish2vec and Thrones2vec
Python Exceptions
Started Learning Alexa skills

# Day 53
Finally figured out seq2seq equation.
Also learned a lot about normal and poisson distributions.
Need to write more code before eod. Slow day.

# Day 54
Finished seq2seq udacity nlpnd project
Added attention_decoder based on keras implementation
TwiML meetup on RNNs

# Day 55
C++ basics
Storytelling with data
  
# Day 56-57
fast.ai v2 lesson 1( Actually 8)
Learnt about building a CNN from scratch and the internal pytorch functions
Learnt about deficiencies of DNN frameworks and why a language like Swift will help solve it
Cpp vectors
Discrete math basics
StoryTelling with data

# Day 58
Created Alexa space facts

# Day 59
Slow day
Convolution math, transposed convolution
Spent day on fast.ai forums
Tried applying for jobs didn''t work
Happy holi

# Day 60
Some revision on RF

# Day 61
DS India meetup on Collaborative Filtering using Fast.ai library
TwiML meetup on fast.ai v2 lesson8 and mini presentations on seq2seq and parallel processing

# Day 62
ASR pipeline
Fourier Transforms
MFCC - Mel Frequency Cepstral Coefficients
Why log
Basic coding in numpy

# Day 63
Fast.ai lesson 1 from scratch
Matrix multiplication and Linear layer from scratch
Kaiming initialization and Xavier initialization

# Day 64
Fast.ai Lesson 9 !st part complete
Why Kaiming normal, Why initialization.
Why scaling is necessary.
Revision of Fully Connected layer
Now onto Training Loop and Learner class

# Day 65
Create a NN with training loop and mini-batches in Pytorch using only tensor apis
Then slowly refactor the code using nn.module, torch.optim, and data set and data loader
Python magic method setattr and repr.
negative log likelihood with softmax = cross_entropy
Integer array indexing
Pytorch nn.module deep dive with add_modules,parameters and nn.sequential
torch.optimizer

# Day 66
Revision of lambda functions
Scopes
Closures
Decorators
Callbacks
Intro - Standard training loop vs fast.ai training loop
A standard abstract class which is inerited by callback implementors
Databunch + Learner

# Day 67
Factory method design pattern
Exploring  Its use in fast.ai callbacks
Callback class using factory methods
Modify training loop accordingly

# Day 68
Ran fit on Mnist after understanding entire training cycle . Kinda revision.
Added Databunch and Combined loss_func,dataloader,accuracy,optimizer into Learner class
Now onto implementing callbacks
Callbacks implemented
Callbacks modified with Runner class.
Now onto Annealing notebook
Meetup of TwiML

# Day 69
Udacity NLP nd
Speech to text project - Using a sonic visualizer
Use of HMM and Cross entropy with N-Grams as a language model to translate speech to text
Started learning MongoDB

# Day 70
Applied for another job.
Pickling and unpickling
Udacity NLPnd final project completed 50%
used setattr to copy object in factory pattern

# Day 71
slow day. Dataclasses and it's uses in python
Trying to do NN using pytorch using callbacks and annealing
Reading more on torch

# Day 72
I created my own github pages today. Still need to do a lot of work though
Finished almost entire notebook of implementing callbacks and annealing
Ready for lesson 10 of fast.ai tomorrow

# Day 73
Started Lesson 10 of fast.ai
Revised Partials , Decorators and Callbacks.
Learnt about Variance and it's notations.
Created partial for class objects as well
  
  
 

     
    
